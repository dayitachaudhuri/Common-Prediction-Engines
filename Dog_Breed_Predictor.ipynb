{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset directory and download the required dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets\n",
    "%cd datasets\n",
    "!mkdir dog_dataset\n",
    "%cd dog_dataset\n",
    "!kaggle datasets list -s dogbreedidfromcomp\n",
    "!kaggle datasets download catherinehorng/dogbreedidfromcomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the Dataset to extract Training and Testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf dogbreedidfromcomp.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"datasets/dog_dataset/{}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read labels from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_all = pd.read_csv(DATASET_PATH.format(\"labels.csv\"))\n",
    "labels_all.head()\n",
    "\n",
    "breeds_all = labels_all[\"breed\"]\n",
    "breed_counts = breeds_all.value_counts()\n",
    "breed_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are finding out the count per class i.e. total data in each class using value_counts() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['scottish_deerhound','maltese_dog','bernese_mountain_dog']\n",
    "labels = labels_all[(labels_all['breed'].isin(CLASS_NAMES))]\n",
    "labels = labels.reset_index()\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating numpy matrix with zeros\n",
    "X_data = np.zeros((len(labels), 224, 224, 3), dtype='float32')\n",
    "# One hot encoding\n",
    "Y_data = label_binarize(labels['breed'], classes = CLASS_NAMES)\n",
    "\n",
    "# Reading and converting image to numpy array and normalizing dataset\n",
    "for i in tqdm(range(len(labels))):\n",
    "    img = image.load_img(DATASET_PATH.format(\"train/%s.jpg\") % labels['id'][i], target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    x = np.expand_dims(img.copy(), axis=0)\n",
    "    X_data[i] = x / 255.0\n",
    "    \n",
    "# Printing train image and one hot encode shape & size\n",
    "print('\\nTrain Images shape: ', X_data.shape,' size: {:,}'.format(X_data.size))\n",
    "print('One-hot encoded output shape: ', Y_data.shape,' size: {:,}'.format(Y_data.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are working with the classification dataset first we need to one hot encode the target value i.e. the classes. After that we will read images and convert them into numpy array and finally normalizing the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5), activation ='relu', input_shape = (224,224,3)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3,3), activation ='relu', kernel_regularizer = 'l2'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (7,7), activation ='relu', kernel_regularizer = 'l2'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters = 8, kernel_size = (5,5), activation ='relu', kernel_regularizer = 'l2'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = \"relu\", kernel_regularizer = 'l2'))\n",
    "model.add(Dense(64, activation = \"relu\", kernel_regularizer = 'l2'))\n",
    "model.add(Dense(len(CLASS_NAMES), activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0001),metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a network architecture for the model. We have used different types of layers according to their features namely Conv_2d (It is used to create a convolutional kernel that is convolved with the input layer to produce the output tensor), max_pooling2d (It is a downsampling technique which takes out the maximum value over the window defined by poolsize), flatten (It flattens the input and creates a 1D output), Dense (Dense layer produce the output as the dot product of input and kernel).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the network architecture we found out the total parameters as 162,619."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data set into training and testing data sets\n",
    "X_train_and_val, X_test, Y_train_and_val, Y_test = train_test_split(X_data, Y_data, test_size = 0.1)\n",
    "# Splitting the training data set into training and validation data sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_and_val, Y_train_and_val, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the network architecture we will start with splitting the test and train data then dividing train data in train and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n",
    "                    validation_data = (X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Accuracy of the model per epoch to analyse how the model is learning with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history.history['accuracy'], color='r')\n",
    "plt.plot(history.history['val_accuracy'], color='b')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'val'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Accuracy over the test set: \\n ', round((score[1]*100), 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use predict function to make predictions using this model also we are finding out the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting image to compare\n",
    "plt.imshow(X_test[1,:,:,:])\n",
    "plt.show()\n",
    "\n",
    "# Finding max value from predition list and comaparing original value vs predicted\n",
    "print(\"Originally : \",labels['breed'][np.argmax(Y_test[1])])\n",
    "print(\"Predicted : \",labels['breed'][np.argmax(Y_pred[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see image with its original and predicted label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
