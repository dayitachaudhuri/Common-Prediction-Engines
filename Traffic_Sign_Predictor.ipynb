{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset directory and download the required dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets\n",
    "%cd datasets\n",
    "!mkdir traffic_sign_dataset\n",
    "%cd traffic_sign_dataset\n",
    "!kaggle datasets list -s gtsrb-german-traffic-sign\n",
    "!kaggle datasets download meowmeowmeowmeowmeow/gtsrb-german-traffic-sign  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the Dataset to extract Training and Testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf dogbreedidfromcomp.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPool2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"datasets/traffic_sign_dataset/{}\"\n",
    "TEST_PATH = DATASET_PATH.format(\"Test\")\n",
    "TRAIN_PATH = DATASET_PATH.format(\"Train\")\n",
    "\n",
    "ALL_LABELS = ['Speed limit (20km/h)','Speed limit (30km/h)','Speed limit (50km/h)','Speed limit (60km/h)',\n",
    "              'Speed limit (70km/h)','Speed limit (80km/h)','End of speed limit (80km/h)','Speed limit (100km/h)',\n",
    "              'Speed limit (120km/h)','No passing','No passing for vechiles over 3.5 metric tons',\n",
    "              'Right-of-way at the next intersection','Priority road','Yield','Stop','No vechiles',\n",
    "              'Vechiles over 3.5 metric tons prohibited','No entry','General caution','Dangerous curve to the left',\n",
    "              'Dangerous curve to the right','Double curve','Bumpy road','Slippery road','Road narrows on the right',\n",
    "              'Road work','Traffic signals','Pedestrians','Children crossing','Bicycles crossing','Beware of ice/snow',\n",
    "              'Wild animals crossing','End of all speed and passing limits','Turn right ahead','Turn left ahead',\n",
    "              'Ahead only','Go straight or right','Go straight or left','Keep right','Keep left','Roundabout mandatory',\n",
    "              'End of no passing','End of no passing by vechiles over 3.5 metric']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize all images to (50,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "label_id = []\n",
    "\n",
    "for i in range(43):\n",
    "    labels = TRAIN_PATH + f\"{i}\"\n",
    "    image_path = os.listdir(labels)\n",
    "    for x in image_path:\n",
    "        img = Image.open(labels + '/' + x)\n",
    "        img = img.resize((50,50))\n",
    "        img = np.array(img)\n",
    "        images.append(img)\n",
    "        label_id.append(i)\n",
    "        \n",
    "images = np.array(images)\n",
    "label_id = np.array(label_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise the pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are finding out the count per class i.e. total data in each class using value_counts() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = pd.DataFrame(label_id).value_counts()\n",
    "label_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test split of data on 80%-20% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(images, label_id , test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the classes column into categorical using to_categorical() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cat = to_categorical(y_train)\n",
    "y_val_cat = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model architecture. In this we will define all the layers with their input shape kernel size, activation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), input_shape = x_train.shape[1:], activation = 'relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(43, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs = 10, batch_size = 128, validation_data = (x_val, y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Accuracy of the model per epoch to analyse how the model is learning with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(model.history.history)\n",
    "evaluation[['accuracy', 'val_accuracy']].plot()\n",
    "evaluation[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use predict function to make predictions using this model also we are finding out the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(test_images, test_path):\n",
    "    images = []\n",
    "\n",
    "    image_path = test_images\n",
    "    \n",
    "    for x in image_path:\n",
    "        img = Image.open(test_path + '/' + x)\n",
    "        img = img.resize((50,50))\n",
    "        img = np.array(img)\n",
    "        images.append(img)\n",
    "\n",
    "    images = np.array(images)\n",
    "    images = images/255\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = scaling(sorted(os.listdir(TEST_PATH)), TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATASET_PATH.format('Test.csv'))\n",
    "y_test = test['ClassId'].values\n",
    "y_pred = model.predict_classes(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(TEST_PATH + '/00001.png')\n",
    "print(\"Original label : \",ALL_LABELS[y_test[1]])\n",
    "print(\"Predicted label : \",ALL_LABELS[y_pred[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
